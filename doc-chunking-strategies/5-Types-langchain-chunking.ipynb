{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Chunking Strategies\n",
        "\n",
        "This notebook demonstrates 5 different types of document chunking strategies using LangChain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Character Text Splitting\n",
        "\n",
        "Basic splitting based on character count with separator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Load the text file\n",
        "loader = TextLoader('SteveJobsSpeech.txt')\n",
        "docs = loader.load()\n",
        "text = docs[0].page_content\n",
        "\n",
        "print(f\"Original text length: {len(text)} characters\")\n",
        "print(f\"First 200 characters: {text[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-based splitting\n",
        "char_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "char_chunks = char_splitter.split_text(text)\n",
        "print(f\"Number of chunks: {len(char_chunks)}\")\n",
        "print(f\"First chunk length: {len(char_chunks[0])} characters\")\n",
        "print(f\"First chunk: {char_chunks[0][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Token-based Chunking\n",
        "\n",
        "Splitting based on token count using tiktoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Token-based splitting\n",
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "token_chunks = token_splitter.split_text(text)\n",
        "print(f\"Number of token-based chunks: {len(token_chunks)}\")\n",
        "print(f\"First token chunk: {token_chunks[0][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Recursive Character Text Splitting\n",
        "\n",
        "Smart splitting that tries to preserve structure by using multiple separators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Recursive character splitting\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "recursive_chunks = recursive_splitter.split_text(text)\n",
        "print(f\"Number of recursive chunks: {len(recursive_chunks)}\")\n",
        "print(f\"First recursive chunk: {recursive_chunks[0][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Markdown Header Text Splitting\n",
        "\n",
        "Splits markdown documents based on header structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "# Load markdown file\n",
        "md_loader = TextLoader('examplemdfile.md')\n",
        "md_docs = md_loader.load()\n",
        "md_text = md_docs[0].page_content\n",
        "\n",
        "# Markdown header splitting\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header1\"),\n",
        "    (\"##\", \"Header2\"),\n",
        "    (\"###\", \"Header3\"),\n",
        "]\n",
        "\n",
        "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_chunks = md_splitter.split_text(md_text)\n",
        "\n",
        "print(f\"Number of markdown chunks: {len(md_chunks)}\")\n",
        "for i, chunk in enumerate(md_chunks[:3]):\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"Metadata: {chunk.metadata}\")\n",
        "    print(f\"Content preview: {chunk.page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Semantic Chunking\n",
        "\n",
        "AI-powered chunking based on semantic similarity using OpenAI embeddings.\n",
        "\n",
        "**Note:** Requires OpenAI API key to be set in environment or passed to OpenAIEmbeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "import os\n",
        "\n",
        "# Semantic chunking (requires OpenAI API key)\n",
        "try:\n",
        "    # Initialize embeddings - add your API key here if not in environment\n",
        "    embeddings = OpenAIEmbeddings()  # or OpenAIEmbeddings(api_key=\"your-key-here\")\n",
        "    \n",
        "    # Create semantic chunker\n",
        "    semantic_chunker = SemanticChunker(embeddings)\n",
        "    \n",
        "    # Split text semantically (using subset for demo)\n",
        "    semantic_chunks = semantic_chunker.split_text(text[:2000])\n",
        "    \n",
        "    print(f\"Number of semantic chunks: {len(semantic_chunks)}\")\n",
        "    for i, chunk in enumerate(semantic_chunks):\n",
        "        print(f\"\\nSemantic Chunk {i+1} (length: {len(chunk)}):\")\n",
        "        print(f\"{chunk[:200]}...\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with semantic chunking: {e}\")\n",
        "    print(\"This likely means the OpenAI API key is missing or invalid.\")\n",
        "    print(\"To use semantic chunking:\")\n",
        "    print(\"1. Get an API key from https://platform.openai.com/api-keys\")\n",
        "    print(\"2. Set it as: export OPENAI_API_KEY='your-key-here'\")\n",
        "    print(\"3. Or pass directly: OpenAIEmbeddings(api_key='your-key-here')\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
