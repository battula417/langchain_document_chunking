{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document Chunking Strategies\n",
        "\n",
        "This notebook demonstrates 5 different types of document chunking strategies using LangChain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Character Text Splitting\n",
        "\n",
        "Basic splitting based on character count with separator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text length: 12098 characters\n",
            "First 200 characters: ‘You’ve got to find what you love,’ Jobs says\n",
            "\n",
            "I’m honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college. A...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Load the text file\n",
        "loader = TextLoader('SteveJobsSpeech.txt')\n",
        "docs = loader.load()\n",
        "text = docs[0].page_content\n",
        "\n",
        "print(f\"Original text length: {len(text)} characters\")\n",
        "print(f\"First 200 characters: {text[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 14\n",
            "First chunk length: 858 characters\n",
            "First chunk: ‘You’ve got to find what you love,’ Jobs says\n",
            "\n",
            "I’m honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college. And this is the closest I’ve ever gotten to a college graduation.\n",
            "\n",
            "Today I want to tell you three sto...\n"
          ]
        }
      ],
      "source": [
        "# Character-based splitting\n",
        "char_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100\n",
        ")\n",
        "\n",
        "char_chunks = char_splitter.split_text(text)\n",
        "print(f\"Number of chunks: {len(char_chunks)}\")\n",
        "print(f\"First chunk length: {len(char_chunks[0])} characters\")\n",
        "print(f\"First chunk: {char_chunks[0][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Token-based Chunking\n",
        "\n",
        "Splitting based on token count using tiktoken.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of token-based chunks: 17\n",
            "First token chunk: y class, and personal computers might not have the wonderful typography that they do. Of course, it was impossible to connect the dots looking forward when I was in college. But it was very, very clear looking backwards, ten years later. Again, you can’t connect the dots looking forward. You can only connect them looking backwards, so you have to trust that the dots will somehow connect in your future.\n",
            "\n",
            "You have to trust in something: your gut, destiny, life, karma, whatever. Because believing t...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Token-based splitting\n",
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "\n",
        "token_chunks = token_splitter.split_text(text)\n",
        "print(f\"Number of token-based chunks: {len(token_chunks)}\")\n",
        "print(f\"First token chunk: {token_chunks[5][:500]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Recursive Character Text Splitting\n",
        "\n",
        "Smart splitting that tries to preserve structure by using multiple separators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of recursive chunks: 14\n",
            "First recursive chunk: ‘You’ve got to find what you love,’ Jobs says\n",
            "\n",
            "I’m honored to be with you today for your commencement from one of the finest universities in the world. Truth be told, I never graduated from college. And this is the closest I’ve ever gotten to a college graduation.\n",
            "\n",
            "Today I want to tell you three sto...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Recursive character splitting\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "recursive_chunks = recursive_splitter.split_text(text)\n",
        "print(f\"Number of recursive chunks: {len(recursive_chunks)}\")\n",
        "print(f\"First recursive chunk: {recursive_chunks[0][:300]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Markdown Header Text Splitting\n",
        "\n",
        "Splits markdown documents based on header structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of markdown chunks: 20\n",
            "\n",
            "Chunk 1:\n",
            "Metadata: {}\n",
            "Content preview: ---\n",
            "__Advertisement :)__  \n",
            "- __[pica](https://nodeca.github.io/pica/demo/)__ - high quality and fast image\n",
            "resize in browser.\n",
            "- __[babelfish](https://github.com/nodeca/babelfish/)__ - developer friend...\n",
            "\n",
            "Chunk 2:\n",
            "Metadata: {'Header1': 'h1 Heading 8-)', 'Header2': 'h2 Heading', 'Header3': 'h3 Heading'}\n",
            "Content preview: #### h4 Heading\n",
            "##### h5 Heading\n",
            "###### h6 Heading...\n",
            "\n",
            "Chunk 3:\n",
            "Metadata: {'Header1': 'h1 Heading 8-)', 'Header2': 'Horizontal Rules'}\n",
            "Content preview: ___  \n",
            "---  \n",
            "***...\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "# Load markdown file\n",
        "md_loader = TextLoader('examplemdfile.md')\n",
        "md_docs = md_loader.load()\n",
        "md_text = md_docs[0].page_content\n",
        "\n",
        "# Markdown header splitting\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header1\"),\n",
        "    (\"##\", \"Header2\"),\n",
        "    (\"###\", \"Header3\"),\n",
        "]\n",
        "\n",
        "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "md_chunks = md_splitter.split_text(md_text)\n",
        "\n",
        "print(f\"Number of markdown chunks: {len(md_chunks)}\")\n",
        "for i, chunk in enumerate(md_chunks[:3]):\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"Metadata: {chunk.metadata}\")\n",
        "    print(f\"Content preview: {chunk.page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Semantic Chunking\n",
        "\n",
        "AI-powered chunking based on semantic similarity using OpenAI embeddings.\n",
        "\n",
        "**Note:** Requires OpenAI API key to be set in environment or passed to OpenAIEmbeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "import os\n",
        "\n",
        "# Initialize embeddings - add your API key here if not in environment\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# Create semantic chunker\n",
        "semantic_chunker = SemanticChunker(embeddings)\n",
        "\n",
        "# Split text semantically (using subset for demo)\n",
        "semantic_chunks = semantic_chunker.split_text(text[:2000])\n",
        "\n",
        "print(f\"Number of semantic chunks: {len(semantic_chunks)}\")\n",
        "for i, chunk in enumerate(semantic_chunks):\n",
        "    print(f\"\\nSemantic Chunk {i+1} (length: {len(chunk)}):\")\n",
        "    print(f\"{chunk[:200]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
